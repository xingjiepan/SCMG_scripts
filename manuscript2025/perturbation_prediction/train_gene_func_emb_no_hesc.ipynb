{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import umap\n",
    "import scipy.stats\n",
    "\n",
    "import torch\n",
    "\n",
    "from scmg.model.contrastive_embedding import (CellEmbedder,  embed_adata)\n",
    "\n",
    "from scmg.preprocessing.data_standardization import GeneNameMapper\n",
    "gene_name_mapper = GeneNameMapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aea852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"figure.autolayout\"] = False\n",
    "matplotlib.rc('pdf', fonttype=42)\n",
    "plt.rcParams['font.family'] = 'FreeSans'\n",
    "sc.set_figure_params(vector_friendly=True, dpi_save=300)\n",
    "plt.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86411267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder model\n",
    "model_ce_path = '../../contrastive_embedding/trained_embedder/'\n",
    "\n",
    "model_ce = torch.load(os.path.join(model_ce_path, 'model.pt'))\n",
    "model_ce.load_state_dict(torch.load(os.path.join(model_ce_path, 'best_state_dict.pth')))\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_ce.to(device)\n",
    "model_ce.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_data_files = [\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/AdamsonWeissman2016_GSM2406681_10X010.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/FrangiehIzar2021_RNA.h5ad',\n",
    "#    '/GPUData_xingjie/SCMG/perturbation_data/hESC_TF_screen.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/JiangSatija2024_IFNB.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/JiangSatija2024_IFNG.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/JiangSatija2024_INS.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/JiangSatija2024_TGFB.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/JiangSatija2024_TNFA.h5ad',\n",
    "#    '/GPUData_xingjie/SCMG/perturbation_data/Joung_TFScreen_HS_2023.h5ad',\n",
    "#    '/GPUData_xingjie/SCMG/perturbation_data/knockTF_human.h5ad',\n",
    "#    '/GPUData_xingjie/SCMG/perturbation_data/knockTF_mouse.h5ad',\n",
    "    #'/GPUData_xingjie/SCMG/perturbation_data/omnipath.h5ad',\n",
    "#    '/GPUData_xingjie/SCMG/perturbation_data/PertOrg.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/ReplogleWeissman2022_K562_essential.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/ReplogleWeissman2022_K562_gwps.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/ReplogleWeissman2022_rpe1.h5ad',\n",
    "#    '/GPUData_xingjie/SCMG/perturbation_data/TianKampmann2021_CRISPRa.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/perturbation_data/TianKampmann2021_CRISPRi.h5ad',\n",
    "    '/GPUData_xingjie/SCMG/hESC_perturb_seq/pseudo_bulk.h5ad', # Test\n",
    "]\n",
    "\n",
    "adata_pert_list = []\n",
    "for pdf in pert_data_files:\n",
    "    adata_pert_list.append(sc.read_h5ad(pdf))\n",
    "    print(os.path.basename(pdf), adata_pert_list[-1].shape[0])\n",
    "\n",
    "adata_pert = anndata.concat(adata_pert_list, axis=0)\n",
    "adata_pert.var['gene_name'] = adata_pert_list[0].var['gene_name']\n",
    "\n",
    "adata_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef30d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_pert = adata_pert[adata_pert.obs['perturbation_sign'] == -1].copy()\n",
    "adata_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out the direct target genes\n",
    "for i in range(adata_pert.shape[0]):\n",
    "    pg = adata_pert.obs['perturbed_gene'].iloc[i]\n",
    "    \n",
    "    if pg in adata_pert.var_names:\n",
    "        adata_pert.X[i, adata_pert.var_names.get_loc(pg)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_pert_ctl = adata_pert.copy()\n",
    "adata_pert_ctl.X = np.exp(adata_pert_ctl.layers['control']) - 1\n",
    "embed_adata(model_ce, adata_pert_ctl, batch_size=8192)\n",
    "\n",
    "adata_pert.obsm['X_ctl_ce_latent'] = adata_pert_ctl.obsm['X_ce_latent']\n",
    "adata_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07453056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch_cat_real_pipeline.py\n",
    "# A simple model & training pipeline: categorical + real vector -> real vector\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def build_vocab(categories: Iterable) -> Dict:\n",
    "    \"\"\"Map each unique category to a contiguous integer id.\"\"\"\n",
    "    uniq = sorted({c for c in categories})\n",
    "    return {c: i for i, c in enumerate(uniq)}\n",
    "\n",
    "\n",
    "def encode_categories(categories: Iterable, vocab: Dict) -> np.ndarray:\n",
    "    \"\"\"Convert a list of categories to integer ids using the vocab.\"\"\"\n",
    "    return np.array([vocab[c] for c in categories], dtype=np.int64)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset\n",
    "# ----------------------------\n",
    "class CatRealDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects:\n",
    "      cats: list/array of length N (strings/ints), one categorical value per row\n",
    "      X: np.ndarray of shape (N, D_in)    - real-valued features\n",
    "      Y: np.ndarray of shape (N, D_out)   - real-valued targets\n",
    "    \"\"\"\n",
    "    def __init__(self, cats: Sequence, X: np.ndarray, Y: np.ndarray, \n",
    "                Y_mask: np.ndarray,\n",
    "                vocab: Dict = None):\n",
    "        assert len(cats) == len(X) == len(Y), \"cats, X, and Y must have the same length\"\n",
    "        assert X.ndim == 2 and Y.ndim == 2, \"X and Y must be 2D arrays\"\n",
    "\n",
    "        self.vocab = build_vocab(cats) if vocab is None else vocab\n",
    "        self.cats = encode_categories(cats, self.vocab)            # (N,)\n",
    "        self.X = X.astype(np.float32)                               # (N, D_in)\n",
    "        self.Y = Y.astype(np.float32)                               # (N, D_out)\n",
    "        self.Y_mask = Y_mask.astype(np.float32)                     # (N, D_out)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return tensors ready for the model\n",
    "        return (\n",
    "            torch.tensor(self.cats[idx], dtype=torch.long),         # categorical id\n",
    "            torch.tensor(self.X[idx], dtype=torch.float32),         # real input vector\n",
    "            torch.tensor(self.Y[idx], dtype=torch.float32),         # real target vector\n",
    "            torch.tensor(self.Y_mask[idx], dtype=torch.float32),   # real target mask\n",
    "        )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Model\n",
    "# ----------------------------\n",
    "class CatRealRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture-of-experts over categorical input:\n",
    "      1) Shared embedding for the category.\n",
    "      2) K linear heads map the embedding -> output vector (experts).\n",
    "      3) A gating MLP consumes the real-valued input and outputs a softmax over heads.\n",
    "      4) Final prediction = weighted sum of the heads with the gate weights.\n",
    "\n",
    "    Args:\n",
    "        num_categories: number of unique categories\n",
    "        real_dim: dimension of real-valued input vector\n",
    "        out_dim: dimension of prediction vector\n",
    "        emb_dim: dimension of category embedding\n",
    "        num_heads: number of expert heads\n",
    "        gate_hidden: hidden size of gating MLP (single hidden layer by default)\n",
    "        dropout: dropout applied on the embedding before experts (optional)\n",
    "        softmax_temp: temperature for the gate softmax (lower -> sharper)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_categories: int,\n",
    "        real_dim: int,\n",
    "        out_dim: int,\n",
    "        emb_dim: int = 16,\n",
    "        num_heads: int = 8,\n",
    "        gate_hidden: int = 32,\n",
    "        dropout: float = 0.0,\n",
    "        softmax_temp: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads >= 2, \"Use at least 2 heads to benefit from gating.\"\n",
    "\n",
    "        # 1) Category embedding\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_categories, embedding_dim=emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()\n",
    "\n",
    "        # 2) Expert linear heads (embedding -> out_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.experts = nn.ModuleList([nn.Linear(emb_dim, out_dim) for _ in range(num_heads)])\n",
    "\n",
    "        # 3) Gating network (real input -> weights over heads)\n",
    "        #    Simple 1-hidden-layer MLP; feel free to deepen if needed.\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(real_dim, gate_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_hidden, num_heads),\n",
    "        )\n",
    "        self.softmax_temp = softmax_temp\n",
    "\n",
    "        # Optional: Initialize experts a bit conservatively\n",
    "        for lin in self.experts:\n",
    "            nn.init.xavier_uniform_(lin.weight)\n",
    "            nn.init.zeros_(lin.bias)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, cat_idx: torch.Tensor, x_real: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            cat_idx: (B,)  long tensor of category ids\n",
    "            x_real:  (B, real_dim) float tensor\n",
    "        Output:\n",
    "            y_hat:   (B, out_dim) float tensor\n",
    "        \"\"\"\n",
    "        B = cat_idx.size(0)\n",
    "\n",
    "        # Embed category and compute each expert head output\n",
    "        emb = self.dropout(self.embedding(cat_idx))             # (B, emb_dim)\n",
    "        head_outs = []\n",
    "        for k in range(self.num_heads):\n",
    "            yk = self.experts[k](emb)                           # (B, out_dim)\n",
    "            head_outs.append(yk)\n",
    "        H = torch.stack(head_outs, dim=1)                       # (B, num_heads, out_dim)\n",
    "\n",
    "        # Gate weights from real input\n",
    "        gate_logits = self.gate(x_real)                         # (B, num_heads)\n",
    "        if self.softmax_temp != 1.0:\n",
    "            gate_logits = gate_logits / self.softmax_temp\n",
    "        w = torch.softmax(gate_logits, dim=1)                   # (B, num_heads)\n",
    "\n",
    "        # Weighted sum of head outputs\n",
    "        y_hat = torch.sum(H * w.unsqueeze(-1), dim=1)           # (B, out_dim)\n",
    "        return y_hat\n",
    "\n",
    "def _mean_pearson_corr(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Column-wise Pearson r between y_true and y_pred, then mean across columns.\n",
    "    Handles constant columns by treating their correlation as 0.0.\n",
    "    y_true, y_pred: (N, D) arrays\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "\n",
    "    # center\n",
    "    yt = y_true - y_true.mean(axis=0, keepdims=True)\n",
    "    yp = y_pred - y_pred.mean(axis=0, keepdims=True)\n",
    "\n",
    "    # std (avoid divide by zero)\n",
    "    syt = np.sqrt((yt ** 2).sum(axis=0) + eps)\n",
    "    syp = np.sqrt((yp ** 2).sum(axis=0) + eps)\n",
    "\n",
    "    # covariance per column\n",
    "    cov = (yt * yp).sum(axis=0)\n",
    "\n",
    "    r = cov / (syt * syp)           # shape (D,)\n",
    "    r = np.clip(r, -1.0, 1.0)       # numeric safety\n",
    "    # If a column is (near) constant in y_true or y_pred, correlation ~0 (already handled via eps)\n",
    "    return float(np.nanmean(r))\n",
    "\n",
    "# ----------------------------\n",
    "# Training / Evaluation\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 20\n",
    "    lr: float = 1e-2\n",
    "    weight_decay: float = 1e-3\n",
    "    val_split: float = 0.0\n",
    "    seed: int = 42\n",
    "    num_workers: int = 0   # set >0 if you want background workers in DataLoader\n",
    "    max_grad_norm: float = 1.0\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    cats: Sequence,\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    Y_mask: np.ndarray,\n",
    "    config: TrainConfig = TrainConfig(),\n",
    "    emb_dim: int = 16,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Trains the model and returns a dict with: model, vocab, history, and device.\n",
    "    \"\"\"\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "\n",
    "    # Dataset & split\n",
    "    full_ds = CatRealDataset(cats, X, Y, Y_mask)\n",
    "    n_total = len(full_ds)\n",
    "    n_val = int(n_total * config.val_split)\n",
    "    n_train = n_total - n_val\n",
    "\n",
    "    if n_val > 0:\n",
    "        train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=torch.Generator().manual_seed(config.seed))\n",
    "        train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "        val_loader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
    "    else:\n",
    "        train_loader = DataLoader(full_ds, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "        val_loader = None\n",
    "\n",
    "    # Model\n",
    "    num_categories = len(full_ds.vocab)\n",
    "    real_dim = X.shape[1]\n",
    "    out_dim = Y.shape[1]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CatRealRegressor(\n",
    "        num_categories=num_categories,\n",
    "        real_dim=real_dim,\n",
    "        out_dim=out_dim,\n",
    "        emb_dim=emb_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss & Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    \n",
    "    history = {\n",
    "        \"train_loss\": [], \"val_loss\": [],\n",
    "        \"val_mae\": [],\n",
    "        \"train_corr\": [], \"val_corr\": []   # <-- new metrics\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "\n",
    "        # for correlation we need full-epoch preds/targets\n",
    "        train_preds_np = []\n",
    "        train_targets_np = []\n",
    "\n",
    "        for cat_ids, x_real, y, y_mask in train_loader:\n",
    "            cat_ids = cat_ids.to(device)\n",
    "            x_real = x_real.to(device)\n",
    "            y = y.to(device)\n",
    "            y_mask = y_mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(cat_ids, x_real)\n",
    "            loss = criterion(preds * y_mask, y * y_mask)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            running += loss.item() * len(x_real)\n",
    "\n",
    "            # collect for corr (detach -> cpu -> numpy)\n",
    "            with torch.no_grad():\n",
    "                train_preds_np.append((preds * y_mask).detach().cpu().numpy())\n",
    "                train_targets_np.append((y * y_mask).detach().cpu().numpy())\n",
    "\n",
    "        train_loss = running / n_train\n",
    "        train_corr = _mean_pearson_corr(np.vstack(train_targets_np), np.vstack(train_preds_np))\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_corr\"].append(train_corr)\n",
    "\n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_running = 0.0\n",
    "            val_mae_running = 0.0\n",
    "            val_preds_np = []\n",
    "            val_targets_np = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for cat_ids, x_real, y, y_mask in val_loader:\n",
    "                    cat_ids = cat_ids.to(device)\n",
    "                    x_real = x_real.to(device)\n",
    "                    y = y.to(device)\n",
    "                    y_mask = y_mask.to(device)\n",
    "\n",
    "                    preds = model(cat_ids, x_real)\n",
    "                    loss = criterion(preds * y_mask, y * y_mask)\n",
    "                    mae = torch.mean(torch.abs(preds - y))\n",
    "\n",
    "                    val_running += loss.item() * len(x_real)\n",
    "                    val_mae_running += mae.item() * len(x_real)\n",
    "\n",
    "                    val_preds_np.append((preds * y_mask).detach().cpu().numpy())\n",
    "                    val_targets_np.append((y * y_mask).detach().cpu().numpy())\n",
    "\n",
    "            val_loss = val_running / max(1, n_val)\n",
    "            val_mae = val_mae_running / max(1, n_val)\n",
    "            val_corr = _mean_pearson_corr(np.vstack(val_targets_np), np.vstack(val_preds_np)) if n_val > 0 else float(\"nan\")\n",
    "\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_mae\"].append(val_mae)\n",
    "            history[\"val_corr\"].append(val_corr)       # <-- saved\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | \"\n",
    "                f\"val_MAE={val_mae:.4f} | train_corr={train_corr:.4f} | val_corr={val_corr:.4f}\"\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"train_loss={train_loss:.4f} | \"\n",
    "                f\"train_corr={train_corr:.4f} | \"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"vocab\": full_ds.vocab,  # category -> id mapping (save this!)\n",
    "        \"history\": history,\n",
    "        \"device\": device,\n",
    "    }\n",
    "\n",
    "\n",
    "from typing import Any, Dict, Iterable, Optional, Sequence, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def predict_cat_real(\n",
    "    model: torch.nn.Module,\n",
    "    vocab: Dict[Any, int],\n",
    "    cats: Union[Sequence[Any], Any, torch.Tensor, np.ndarray],\n",
    "    X: Union[np.ndarray, torch.Tensor],\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    *,\n",
    "    unknown_category: str = \"error\",   # \"error\" | \"use_zero\" | \"add_oov\"\n",
    "    return_torch: bool = False,\n",
    "    batch_size: Optional[int] = None,  # if set, runs minibatched inference\n",
    ") -> Union[np.ndarray, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Inference for CatRealRegressor.\n",
    "\n",
    "    Args:\n",
    "        model: trained CatRealRegressor (or loaded state_dict into same class)\n",
    "        vocab: mapping category_value -> category_id (returned by train_model)\n",
    "        cats: category labels (list/array) OR a single label OR encoded ids tensor/array\n",
    "        X: real-valued inputs, shape (N, real_dim) or (real_dim,) for single example\n",
    "        device: override device; if None, inferred from model parameters\n",
    "        unknown_category:\n",
    "            - \"error\": raise KeyError if a category not in vocab is encountered\n",
    "            - \"use_zero\": map unknown categories to id 0\n",
    "            - \"add_oov\": map unknown categories to a dedicated OOV id at the end\n",
    "              (NOTE: only works if the model was trained with that extra embedding row)\n",
    "        return_torch: return torch.Tensor instead of np.ndarray\n",
    "        batch_size: if provided, process in batches to reduce memory usage\n",
    "\n",
    "    Returns:\n",
    "        y_pred: (N, out_dim) predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Decide device\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    # Normalize X to 2D\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X_np = X\n",
    "        if X_np.ndim == 1:\n",
    "            X_np = X_np[None, :]\n",
    "        x_t = torch.from_numpy(X_np.astype(np.float32))\n",
    "    elif torch.is_tensor(X):\n",
    "        x_t = X\n",
    "        if x_t.ndim == 1:\n",
    "            x_t = x_t.unsqueeze(0)\n",
    "        x_t = x_t.to(dtype=torch.float32)\n",
    "    else:\n",
    "        raise TypeError(\"X must be a numpy array or torch tensor\")\n",
    "\n",
    "    n = x_t.shape[0]\n",
    "\n",
    "    # Normalize cats to 1D ids tensor\n",
    "    def _encode_one(c):\n",
    "        if c in vocab:\n",
    "            return vocab[c]\n",
    "        if unknown_category == \"use_zero\":\n",
    "            return 0\n",
    "        if unknown_category == \"add_oov\":\n",
    "            return len(vocab)  # requires model.embedding.num_embeddings == len(vocab)+1\n",
    "        raise KeyError(f\"Unknown category {c!r}. Set unknown_category to handle it.\")\n",
    "\n",
    "    if torch.is_tensor(cats):\n",
    "        cat_ids_t = cats.to(dtype=torch.long)\n",
    "        if cat_ids_t.ndim == 0:\n",
    "            cat_ids_t = cat_ids_t.unsqueeze(0)\n",
    "        if cat_ids_t.ndim != 1:\n",
    "            cat_ids_t = cat_ids_t.view(-1)\n",
    "    elif isinstance(cats, np.ndarray):\n",
    "        if np.issubdtype(cats.dtype, np.integer):\n",
    "            cat_ids_t = torch.from_numpy(cats.astype(np.int64)).view(-1)\n",
    "        else:\n",
    "            # object/str array\n",
    "            flat = cats.reshape(-1).tolist()\n",
    "            ids = [_encode_one(c) for c in flat]\n",
    "            cat_ids_t = torch.tensor(ids, dtype=torch.long)\n",
    "    else:\n",
    "        # Python scalar or sequence\n",
    "        if isinstance(cats, (str, int)) or not isinstance(cats, Sequence):\n",
    "            cats_list = [cats]\n",
    "        else:\n",
    "            cats_list = list(cats)\n",
    "        ids = [_encode_one(c) for c in cats_list]\n",
    "        cat_ids_t = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    if cat_ids_t.numel() == 1 and n > 1:\n",
    "        # broadcast single category across batch\n",
    "        cat_ids_t = cat_ids_t.expand(n)\n",
    "    if cat_ids_t.numel() != n:\n",
    "        raise ValueError(f\"cats length ({cat_ids_t.numel()}) must match X batch size ({n}).\")\n",
    "\n",
    "    # Move to device\n",
    "    x_t = x_t.to(device)\n",
    "    cat_ids_t = cat_ids_t.to(device)\n",
    "\n",
    "    # Run inference (optionally minibatched)\n",
    "    with torch.no_grad():\n",
    "        if batch_size is None:\n",
    "            y = model(cat_ids_t, x_t)\n",
    "        else:\n",
    "            outs = []\n",
    "            for i in range(0, n, batch_size):\n",
    "                outs.append(model(cat_ids_t[i:i+batch_size], x_t[i:i+batch_size]))\n",
    "            y = torch.cat(outs, dim=0)\n",
    "\n",
    "    if return_torch:\n",
    "        return y\n",
    "    return y.detach().cpu().numpy()\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pearson_per_row(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    mask: Optional[np.ndarray] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation per row between y_true and y_pred.\n",
    "\n",
    "    Args:\n",
    "        y_true: (N, D) true targets\n",
    "        y_pred: (N, D) predicted targets\n",
    "        mask:   optional boolean or {0,1} array of shape (N, D).\n",
    "                If provided, correlation for each row is computed only on masked-in dims.\n",
    "\n",
    "    Returns:\n",
    "        r: (N,) array of Pearson r values. Rows with <2 valid dims or zero variance return np.nan.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "\n",
    "    if y_true.shape != y_pred.shape or y_true.ndim != 2:\n",
    "        raise ValueError(f\"Expected y_true and y_pred to both be (N, D). Got {y_true.shape} vs {y_pred.shape}.\")\n",
    "\n",
    "    N, D = y_true.shape\n",
    "\n",
    "    if mask is None:\n",
    "        valid = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    else:\n",
    "        m = np.asarray(mask).astype(bool)\n",
    "        if m.shape != (N, D):\n",
    "            raise ValueError(f\"mask must be shape (N, D). Got {m.shape}.\")\n",
    "        valid = m & np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "\n",
    "    r = np.full(N, np.nan, dtype=np.float64)\n",
    "\n",
    "    for i in range(N):\n",
    "        idx = valid[i]\n",
    "        if idx.sum() < 2:\n",
    "            continue\n",
    "\n",
    "        a = y_true[i, idx]\n",
    "        b = y_pred[i, idx]\n",
    "\n",
    "        a_mean = a.mean()\n",
    "        b_mean = b.mean()\n",
    "        a0 = a - a_mean\n",
    "        b0 = b - b_mean\n",
    "\n",
    "        denom = np.sqrt((a0 * a0).sum()) * np.sqrt((b0 * b0).sum())\n",
    "        if denom == 0:\n",
    "            continue\n",
    "\n",
    "        r[i] = (a0 * b0).sum() / denom\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def plot_corr_distribution(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    mask: Optional[np.ndarray] = None,\n",
    "    bins: int = 50,\n",
    "    title: str = \"Per-sample Pearson correlation (y_true vs y_pred)\",\n",
    ") -> Tuple[np.ndarray, plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    Computes per-row Pearson correlations and plots their distribution (histogram).\n",
    "\n",
    "    Returns:\n",
    "        (r, fig, ax)\n",
    "    \"\"\"\n",
    "    r = pearson_per_row(y_true, y_pred, mask=mask)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    finite_r = r[np.isfinite(r)]\n",
    "\n",
    "    ax.hist(finite_r, bins=bins)\n",
    "    ax.set_xlabel(\"Pearson r\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Helpful summary markers (no forced colors)\n",
    "    if finite_r.size:\n",
    "        ax.axvline(np.nanmean(finite_r), linewidth=2, linestyle=\"--\", label=f\"mean={np.nanmean(finite_r):.3f}\")\n",
    "        ax.axvline(np.nanmedian(finite_r), linewidth=2, linestyle=\":\", label=f\"median={np.nanmedian(finite_r):.3f}\")\n",
    "        ax.legend()\n",
    "\n",
    "    return r, fig, ax\n",
    "\n",
    "\n",
    "# Example:\n",
    "# r, fig, ax = plot_corr_distribution(Y_true, Y_pred, mask=Y_mask)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_pert.obs['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec654863",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_train = adata_pert[adata_pert.obs['condition'] != 'hESC_perturb_seq'].copy()\n",
    "\n",
    "adata_test = adata_pert[adata_pert.obs['condition'] == 'hESC_perturb_seq'].copy()\n",
    "adata_test = adata_test[adata_test.obs['perturbed_gene_name'].isin(adata_train.obs['perturbed_gene_name'])].copy()\n",
    "#adata_test = adata_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainConfig(epochs=100, batch_size=256, lr=1e-2)\n",
    "\n",
    "cats = list(adata_train.obs['perturbed_gene_name'].values)\n",
    "X = adata_train.obsm['X_ctl_ce_latent'].copy()\n",
    "Y = adata_train.X.copy()\n",
    "Y_mask = adata_train.layers['measure_mask'].copy()\n",
    "\n",
    "result = train_model(cats, X, Y, Y_mask, config=cfg, emb_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test.layers['zero_shot_pred'] = predict_cat_real(result['model'], result['vocab'], \n",
    "                 list(adata_test.obs['perturbed_gene_name'].values), adata_test.obsm['X_ctl_ce_latent'].copy())\n",
    "adata_test.write_h5ad('zero_shot_pred_hESC.h5ad')\n",
    "\n",
    "plot_corr_distribution(adata_test.X, adata_test.layers['zero_shot_pred'] )\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76de544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history: dict):\n",
    "    \"\"\"\n",
    "    Plot training/validation loss, MAE, and correlation curves.\n",
    "    Expects keys:\n",
    "      - 'train_loss', 'val_loss'\n",
    "      - 'val_mae'\n",
    "      - 'train_corr', 'val_corr'\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(9, 4))\n",
    "\n",
    "    # ---- Loss subplot ----\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "    if len(history['train_loss']) == len(history['val_loss']):\n",
    "        plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
    "    \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # ---- Correlation subplot ----\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history[\"train_corr\"], label=\"Train Corr\", color=\"green\")\n",
    "    if len(history['train_loss']) == len(history['val_loss']):\n",
    "        plt.plot(epochs, history[\"val_corr\"], label=\"Val Corr\", color=\"red\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Pearson r\")\n",
    "    plt.title(\"Correlation\")\n",
    "    #plt.ylim(-1.0, 1.0)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "     # ---- MAE subplot ----\n",
    "    if len(history['train_loss']) == len(history['val_loss']):\n",
    "        plt.plot(epochs, history[\"val_mae\"], marker=\"o\", label=\"Val MAE\", color=\"orange\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Mean Absolute Error\")\n",
    "        plt.title(\"Validation MAE\")\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(result['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_embeddings(model, vocab: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract category embeddings as a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        model: trained model that has `embedding` layer\n",
    "        vocab: dict mapping category_name -> id (from dataset)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame of shape (num_categories, emb_dim), \n",
    "        with category names as index.\n",
    "    \"\"\"\n",
    "    # Get weight matrix (num_categories x emb_dim)\n",
    "    emb_matrix = model.embedding.weight.detach().cpu().numpy()\n",
    "    \n",
    "    # Reverse vocab: id -> category\n",
    "    id_to_cat = {i: c for c, i in vocab.items()}\n",
    "    \n",
    "    # Build dataframe\n",
    "    df = pd.DataFrame(\n",
    "        emb_matrix,\n",
    "        index=[id_to_cat[i] for i in range(len(id_to_cat))],\n",
    "        columns=[f\"dim_{j}\" for j in range(emb_matrix.shape[1])]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "emb_df = get_category_embeddings(result[\"model\"], result[\"vocab\"])\n",
    "emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df.to_parquet('gene_func_emb_no_hesc.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "def get_output_feature_embeddings(\n",
    "    model,\n",
    "    out_feature_names: Optional[List[str]] = None,\n",
    "    normalize: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build output-feature embeddings by concatenating the per-expert weight rows.\n",
    "\n",
    "    For each expert k (Linear: out_dim x emb_dim), take row j -> vector in R^{emb_dim}.\n",
    "    Concatenate across all experts to get a vector in R^{emb_dim * num_heads} for output j.\n",
    "\n",
    "    Args:\n",
    "        model: Trained CatRealRegressor with `experts` (ModuleList of Linear layers).\n",
    "        out_feature_names: Optional list of names for outputs (length = out_dim).\n",
    "                           If None, uses ['out_0', ..., 'out_{out_dim-1}'].\n",
    "        normalize: If True, L2-normalize each embedding row after concatenation.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame of shape (out_dim, emb_dim * num_heads), indexed by output names.\n",
    "    \"\"\"\n",
    "    # Grab expert weights as numpy arrays; each W_k has shape (out_dim, emb_dim)\n",
    "    expert_weights = [lin.weight.detach().cpu().numpy() for lin in model.experts]\n",
    "    num_heads = len(expert_weights)\n",
    "    out_dim, emb_dim = expert_weights[0].shape\n",
    "\n",
    "    # Sanity check: consistent shapes across experts\n",
    "    for W in expert_weights[1:]:\n",
    "        assert W.shape == (out_dim, emb_dim), \"All experts must have the same (out_dim, emb_dim).\"\n",
    "\n",
    "    # For each output feature j, collect [W_0[j,:], W_1[j,:], ..., W_{K-1}[j,:]] and concat\n",
    "    rows = []\n",
    "    for j in range(out_dim):\n",
    "        parts = [W[j, :] for W in expert_weights]                   # K x emb_dim\n",
    "        emb = np.concatenate(parts, axis=0)                         # (K*emb_dim,)\n",
    "        if normalize:\n",
    "            norm = np.linalg.norm(emb) + 1e-12\n",
    "            emb = emb / norm\n",
    "        rows.append(emb)\n",
    "\n",
    "    # Build column names for interpretability: head_k.dim_d\n",
    "    col_names = [f\"head_{k}.dim_{d}\" for k in range(num_heads) for d in range(emb_dim)]\n",
    "\n",
    "    # Output names\n",
    "    if out_feature_names is None:\n",
    "        out_feature_names = [f\"out_{j}\" for j in range(out_dim)]\n",
    "    else:\n",
    "        assert len(out_feature_names) == out_dim, \"out_feature_names length must match out_dim.\"\n",
    "\n",
    "    df = pd.DataFrame(rows, index=out_feature_names, columns=col_names)\n",
    "    return df\n",
    "\n",
    "out_emb_df = get_output_feature_embeddings(result[\"model\"], out_feature_names=list(adata_pert.var.index), normalize=False)\n",
    "out_emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757139e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
