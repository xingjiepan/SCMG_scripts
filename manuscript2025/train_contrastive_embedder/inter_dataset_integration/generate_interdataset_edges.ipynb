{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "sc.settings.n_jobs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hv_genes(adata_origin):\n",
    "    adata = adata_origin.copy()\n",
    "    sc.pp.filter_cells(adata, min_genes=20)\n",
    "\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.01, max_mean=10, min_disp=0.3)\n",
    "\n",
    "    return adata.var[adata.var['highly_variable']].index.tolist()\n",
    "\n",
    "def find_common_hv_genes(adata1, adata2):\n",
    "    adata_merge = anndata.concat(\n",
    "        {'b1': adata1, 'b2': adata2},\n",
    "        label='batch_category',\n",
    "    )\n",
    "    sc.pp.filter_cells(adata_merge, min_genes=20)\n",
    "\n",
    "    sc.pp.normalize_total(adata_merge, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_merge)\n",
    "    sc.pp.highly_variable_genes(adata_merge, \n",
    "                    n_top_genes=2000, batch_key='batch_category')\n",
    "    return adata_merge.var[adata_merge.var['highly_variable']].index.tolist()\n",
    "\n",
    "def get_merged_datasets(adata1, adata2): \n",
    "    adata1.var.index = list(adata1.var['human_gene_id'])\n",
    "    adata2.var.index = list(adata2.var['human_gene_id'])\n",
    "    adata1.var_names_make_unique()\n",
    "    adata2.var_names_make_unique()\n",
    "\n",
    "    common_genes = np.intersect1d(adata1.var_names, adata2.var_names)\n",
    "    adata1 = adata1[:, common_genes].copy()\n",
    "    adata2 = adata2[:, common_genes].copy()\n",
    "\n",
    "    sc.pp.filter_cells(adata1, min_genes=20)\n",
    "    sc.pp.filter_cells(adata2, min_genes=20)\n",
    "\n",
    "    if (adata1.shape[0] < 10) or (adata2.shape[0] < 10):\n",
    "        return None, None\n",
    "\n",
    "    adata_merge = anndata.concat(\n",
    "        {'b1': adata1, 'b2': adata2},\n",
    "        label='batch_category',\n",
    "    )\n",
    "\n",
    "    sc.pp.normalize_total(adata_merge, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_merge)\n",
    "\n",
    "    sc.pp.highly_variable_genes(adata_merge, \n",
    "                    n_top_genes=2000, batch_key='batch_category')\n",
    "    adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy()\n",
    "    sc.pp.scale(adata_merge, max_value=100)\n",
    "\n",
    "    # Get the significant PCs\n",
    "    sc.tl.pca(adata_merge, svd_solver='arpack', n_comps=min(100, adata_merge.n_obs - 1))\n",
    "    adata_merge.obsm['X_pca_norm'] = adata_merge.obsm['X_pca'] / (np.linalg.norm(\n",
    "        adata_merge.obsm['X_pca'], axis=1) + 1e-6)[:, None]\n",
    "\n",
    "    adata_m1 = adata_merge[adata_merge.obs['batch_category'] == 'b1']\n",
    "    adata_m2 = adata_merge[adata_merge.obs['batch_category'] == 'b2']\n",
    "\n",
    "    return adata_m1, adata_m2\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def mutual_k_nearest_neighbors(arr1, arr2, k1, k2, metric='euclidean'):\n",
    "    # Initialize NearestNeighbors models for each array\n",
    "    nn1 = NearestNeighbors(n_neighbors=k2, metric=metric)\n",
    "    nn2 = NearestNeighbors(n_neighbors=k1, metric=metric)\n",
    "    \n",
    "    # Fit the models\n",
    "    nn1.fit(arr1)\n",
    "    nn2.fit(arr2)\n",
    "    \n",
    "    # Find K nearest neighbors\n",
    "    distances1, indices1 = nn2.kneighbors(arr1)\n",
    "    distances2, indices2 = nn1.kneighbors(arr2)\n",
    "    \n",
    "    p1s = []\n",
    "    p2s = []\n",
    "    distances = []\n",
    "    \n",
    "    # Iterate over points in arr1\n",
    "    for p1 in range(arr1.shape[0]):\n",
    "        for i in range(k1):\n",
    "            p2 = indices1[p1, i]\n",
    "            if p1 in indices2[p2]:\n",
    "                p1s.append(p1)\n",
    "                p2s.append(p2)\n",
    "                distances.append(distances1[p1, i])\n",
    "    \n",
    "    return p1s, p2s, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../integration_specs.json' ,'r') as f:\n",
    "    i_specs_d = json.load(f)\n",
    "\n",
    "all_ref_datasets = set()\n",
    "for v in i_specs_d.values():\n",
    "    for d in v:\n",
    "        all_ref_datasets.add(d.split('@')[1])\n",
    "\n",
    "all_ref_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "input_path = '/GPUData_xingjie/SCMG/sc_rna_data'\n",
    "\n",
    "# Create the output folder\n",
    "output_path = '/GPUData_xingjie/SCMG/contrastive_embedding_training/edges/inter_dataset/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Generate the training datasets\n",
    "for ds_ref in sorted(all_ref_datasets):\n",
    "    ds_ref_name = ds_ref.replace(':', '_')\n",
    "\n",
    "    # Define all the query datasets\n",
    "    query_datasets = set()\n",
    "    for k, v in i_specs_d.items():\n",
    "        for d in v:\n",
    "            if d.split('@')[1] == ds_ref:\n",
    "\n",
    "                # Do not overwrite\n",
    "                ds_ref_name = ds_ref.replace(':', '_')\n",
    "                ds_query_name = k.split('@')[1].replace(':', '_')\n",
    "                if os.path.exists(os.path.join(output_path, \n",
    "                        f'{ds_ref_name}_AND_{ds_query_name}.parquet')):\n",
    "                    continue\n",
    "\n",
    "                query_datasets.add(k.split('@')[1])\n",
    "\n",
    "    if len(query_datasets) == 0:\n",
    "        continue\n",
    "\n",
    "    adata_ref_all = sc.read_h5ad(\n",
    "        os.path.join(input_path, f'standard_adata_{ds_ref_name}.h5ad'))\n",
    "\n",
    "    # Pairwise integration\n",
    "    for ds_query in sorted(query_datasets):\n",
    "        if ds_query == ds_ref:\n",
    "            continue\n",
    "\n",
    "        ds_query_name = ds_query.replace(':', '_')\n",
    "\n",
    "        print(f'\\nIntegrate {ds_ref} and {ds_query}')\n",
    "\n",
    "        adata_query_all = sc.read_h5ad(\n",
    "            os.path.join(input_path, f'standard_adata_{ds_query_name}.h5ad'))\n",
    "        \n",
    "        # Define the integratable ref and query cell types\n",
    "        query_ct_map = {}\n",
    "        for k, v in i_specs_d.items():\n",
    "            if k.split('@')[1] != ds_query:\n",
    "                continue\n",
    "            for d in v:\n",
    "                if d.split('@')[1] == ds_ref:\n",
    "                    if k.split('@')[0] not in query_ct_map:\n",
    "                        query_ct_map[k.split('@')[0]] = []\n",
    "\n",
    "                    query_ct_map[k.split('@')[0]].append(d.split('@')[0])\n",
    "\n",
    "        selected_query_cts = np.array(list(query_ct_map.keys()))\n",
    "        selected_ref_cts = np.array(list(set(v for l in query_ct_map.values()\n",
    "                                    for v in l)))\n",
    "        print(f'Selected query cell types: {selected_query_cts}')\n",
    "        print(f'Selected ref cell types: {selected_ref_cts}')\n",
    "\n",
    "        adata_ref = adata_ref_all[\n",
    "            adata_ref_all.obs['cell_type'].isin(selected_ref_cts)].copy()\n",
    "        adata_query = adata_query_all[\n",
    "            adata_query_all.obs['cell_type'].isin(selected_query_cts)].copy()\n",
    "        \n",
    "        if adata_ref.shape[0] < 5 or adata_query.shape[0] < 5:\n",
    "            continue\n",
    "\n",
    "        # Merge the datasets\n",
    "        adata_m_ref, adata_m_query = get_merged_datasets(adata_ref, adata_query)\n",
    "        \n",
    "        if adata_m_ref is None or adata_m_query is None:\n",
    "            continue\n",
    "        if adata_m_ref.shape[0] < 1 or adata_m_query.shape[0] < 1:\n",
    "            continue\n",
    "\n",
    "        print(f'Query dataset has {adata_m_query.shape[0]} cells.')\n",
    "        print(f'Reference dataset has {adata_m_ref.shape[0]} cells.')\n",
    "\n",
    "        # Integrated for two datasets\n",
    "        cell_pairs_d = {\n",
    "            'cell_ref': [],\n",
    "            'cell_query': [],\n",
    "            'dataset_ref': [],\n",
    "            'dataset_query': [],\n",
    "            'cell_type_ref' : [],\n",
    "            'cell_type_query' : [],\n",
    "        }\n",
    "\n",
    "        k1 = min(10, adata_m_ref.shape[0])\n",
    "        k2 = min(10, adata_m_query.shape[0])\n",
    "\n",
    "        p1s, p2s, distances = mutual_k_nearest_neighbors(\n",
    "                        adata_m_query.obsm['X_pca_norm'], \n",
    "                        adata_m_ref.obsm['X_pca_norm'], \n",
    "                        k1=k1, k2=k2, metric='euclidean')\n",
    "        \n",
    "        cell_pairs_d['cell_query'].extend(adata_m_query.obs.index[p1s])\n",
    "        cell_pairs_d['cell_ref'].extend(adata_m_ref.obs.index[p2s])\n",
    "        cell_pairs_d['cell_type_query'].extend(adata_m_query.obs['cell_type'].values[p1s])\n",
    "        cell_pairs_d['cell_type_ref'].extend(adata_m_ref.obs['cell_type'].values[p2s])\n",
    "        cell_pairs_d['dataset_query'].extend(adata_m_query.obs['dataset_id'].values[p1s])\n",
    "        cell_pairs_d['dataset_ref'].extend(adata_m_ref.obs['dataset_id'].values[p2s])\n",
    "\n",
    "        mkn_df = pd.DataFrame(cell_pairs_d)\n",
    "\n",
    "        # Filter out cell pairs that are not allowed\n",
    "        filter_mask = []\n",
    "        for i, row in mkn_df.iterrows():\n",
    "            if row['cell_type_query'] in query_ct_map:\n",
    "                if row['cell_type_ref'] in query_ct_map[row['cell_type_query']]:\n",
    "                    filter_mask.append(True)\n",
    "                else:\n",
    "                    filter_mask.append(False)\n",
    "            else:\n",
    "                filter_mask.append(False)\n",
    "\n",
    "        mkn_df = mkn_df[filter_mask]       \n",
    "       \n",
    "        print(f'Found {mkn_df.shape[0]} mutual k-nearest neighbors.')\n",
    "        mkn_df.to_parquet(os.path.join(output_path, \n",
    "                        f'{ds_ref_name}_AND_{ds_query_name}.parquet'))\n",
    "        \n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
